\documentclass{article}

% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2020

% ready for submission
% \usepackage{neurips_2020}

% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2020}

% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2020}

% to avoid loading the natbib package, add option nonatbib:
\usepackage[nonatbib]{neurips_2020}
\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography

\usepackage{amsmath}
\usepackage{makecell}

\DeclareMathOperator*{\argmax}{arg\,max}
\DeclareMathOperator*{\argmin}{arg\,min}

\title{Do (wo)men talk too much in films?}

% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.

\author{%
  Johannes Graner \\
  % examples of more authors
  \And
  Isak Dahlqvist \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  \AND
  William Norman \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

\begin{document}

\maketitle

\begin{abstract}
  
\end{abstract}

\newpage

\section{Introduction}

\section{Methods}
We have chosen to focus on approaches using logistic regression, k-NN and LDA/QDA to classify the lead actor's gender.

In order to make the methods as comparable as possible, we have used a common set of transformations of the input variables for all tested methods.

\subsection{Input transformations}
In the given dataset, there are columns for the total number of words spoken as well as the number of words spoken by the lead, the co-lead etc. This could present a problem since if we compare a movie where the lead says 10 out of 100 total words and another movie where the lead says 100 out of 1000 words, most models would think that the lead speaks more in the second movie and miss the fact that the \textit{proportion} of words spoken by the lead is the same. For that reason we have transformed several input variables to express a proportion instead of absolute numbers. We also believe it might be important to have a dummy variable indicating if the lead or the co-lead is oldest. All transformations are given in Table \ref{tab:transformations}.

\begin{table}[h!]
	\centering
	\begin{tabular}{ccc}
		Original column & New column & Transformation \\
		\midrule
		Number of words lead & \makecell{Proportion of \\ words lead} & $\frac{\text{Number of words lead}}{\text{Total words}}$ \\
		N/A & \makecell{Proportion of \\ words co-lead} & $\frac{\text{Number of words lead - Difference in words lead and co-lead}}{\text{Total words}}$ \\
		\makecell{Difference in words \\ lead and co-lead} & \makecell{Ratio words \\ co-lead lead} & $\frac{\text{Proportion of words co-lead}}{\text{Proportion of words lead}}$ \\
		Number words female & \makecell{Proportion of \\ words female} & $\frac{\text{Number words female}}{\text{Total words - Number of words lead}}$ \\
		\makecell{Number of \\ female actors} & \makecell{Proportion of \\ female actors} & $\frac{\text{Number of female actors}}{\text{Number of female actors + Number of male actos}}$ \\
		\makecell{Number of \\ male actors} & \makecell{Number of actors} & \makecell{Number of male actors + \\ Number of female actors} \\
		N/A & Older lead & $\begin{cases} 1, \text{Age lead > Age Co-Lead} \\ 0, \text{else} \end{cases}$
	\end{tabular}
	\label{tab:transformations}
	\caption{Transformations of input variables.}
\end{table}

Note that when determining 'Proportion of words female', this should only measure the words spoken by non-lead female actors so we have to subtract the lead's contribution to the total number of words.

The column 'Number of male actors' was dropped since all necessary information in this column is contained in 'Proportion of female actors' together with 'Number of actors'.

In order to improve regularization and k-NN, all remaining numerical input variables where centered and scaled by their standard deviation. This means that columns with proportions have values in the unit interval $[0,1]$ and the other numerical variables have values that are of roughly the same magnitude.

\subsection{Logistic Regression}
Logistic regression is a \textit{general linear model} (GLM), i.e. the relationship between the data $X \in \mathcal{X} \subseteq \mathbb{R}^p$ and the outcome $Y$ is on the form
\begin{equation}
	E(Y|X) = g^{-1}(X \cdot \beta)
\end{equation}
where $\beta \in \mathbb{R}^p$ and $g$ is the link function. In the case of logistic regression, $Y|X \sim Ber(p)$ and the canonical link function is the logit link $g(x) = \log \left( \frac{x}{1 - x} \right)$ with $g^{-1}(x) = \frac{\exp(x)}{1 + \exp(x)}$. Since $Y|X \sim Ber(p)$, we get $E(Y|X) = p = g^{-1}(X \cdot \beta)$. In other words, $P(Y = 1 | X = x) = g^{-1}(x \cdot \beta)$, which we can use to predict $Y$ given data $x$.

To do the regression, we find $\hat \beta \in \argmin_\beta \sum_{i=1}^{n} (y_i - \hat y(x_i; \beta))^2$ where $\hat y(x;\beta) = g^{-1} (x \cdot \beta)$. This minimizes the mean squared error (MSE) loss function. A potential problem with this approach is that there are no restrictions on the components of $\beta$ and that can lead to overfitting, especially if $n$ is not much larger than $p$. To address that issue, one can introduce regularization.

In general, regularization is done by adding a penalizing term to the loss function that restricts $\beta$ in some way. If $L(\beta; x_i,y_i)$ is the loss function before regularization, we instead consider the new loss function $L(\beta; x_i,y_i) + \lambda R(\beta)$ and find $\hat \beta_{reg} \in \argmin_\beta \left( L(\beta; x_i, y_i) + \lambda R(\beta) \right)$. $R$ is some penalizing function and $\lambda$ is a hyper-parameter that can be tuned. The two most common forms of regularization is LASSO and Ridge regression.

LASSO regression uses $L_1$-regularization, meaning that $R_{LASSO}(\beta) = ||\beta||_1 =  \sum_{i=1}^{p} |\beta_i|$ while Ridge regression uses $L_2$-regularization, $R_{Ridge}(\beta) = ||\beta||_2^2 = \sum_{i=1}^{p} \beta_i^2$.

In order to find a value of $\lambda$ that performs well on the data, cross-validation is used to find the optimal value in a finite set $\Lambda = \{ \lambda_1,\dots,\lambda_k \}$. Cross-validation works by splitting the data into $n$ equally sized partitions and training the data separately on the $n$ choices of $n-1$ partitions and testing on the partition that was left out. The test error $E_{new}$ is estimated by the mean misclassification rate across the partitions. This procedure is repeated for each $\lambda_j \in \Lambda$ and the value resulting in the lowest estimated test error is chosen.

Since cross-validation is used to estimate the hyper-parameter $\lambda$, this method cannot be used to estimate the test error of the whole procedure. Instead, the dataset has to be split into a training set and a testing set with a specified fraction of the total data in each set. The whole procedure above is done on the training set and the test error is estimated by evaluating the performance of the model on the testing set. However, this can yield significantly different estimates of the test error since only one split into training and testing data is considered. To get a better estimate of the actual testing error, a bootstrap procedure is performed.

Since the full dataset is an iid sample from some unknown distribution, the estimated test error $\hat E_{new}$ is a random variable. By repeating the whole procedure $B$ times (i.e. $B$ independent splits into training and testing data and subsequent fitting and cross-validation), a bootstrap sample of $\hat E_{new}$ is obtained which can be used to estimate the distribution (or at least properties thereof) of $\hat E_{new}$. This is very computationally intensive but gives a much clearer view of the variability of the test error compared to just computing it for one split.

\subsection{k-Nearest Neighbors}

\subsection{LDA and QDA}

\section{Results}

\subsection{Logistic Regression}



\subsubsection{k-Nearest Neighbors}

\subsection{LDA and QDA}

\section{Conclusions}

\section{Feature Importance}

\end{document}
