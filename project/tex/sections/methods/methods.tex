\documentclass[../../project.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}
	We have chosen to focus on approaches using logistic regression, k-NN, LDA and QDA to classify the lead actor's gender.
	
	In order to make the methods as comparable as possible, we have used a common set of transformations of the input variables for all tested methods.
	
	To compare between families of models and between which tuning is better we chose to focus on two measures: accuracy (on average, of how often model makes a correct prediction) and AUC (area under ROC curve).
	
	Several methods have a  hyper-parameter $\lambda$ that needs to be tuned. In order to find a value of $\lambda$ that performs well on the data, cross-validation is used to find the optimal value in a finite set $\Lambda = \{ \lambda_1,\dots,\lambda_k \}$. Cross-validation works by splitting the data into $n$ equally sized partitions and training the data separately on the $n$ choices of $n-1$ partitions and testing on the partition that was left out. The test error $E_{new}$ is estimated by the mean misclassification rate across the partitions. This procedure is repeated for each $\lambda_j \in \Lambda$ and the value resulting in the lowest estimated test error is chosen.
	
	Since cross-validation is used to estimate the hyper-parameter $\lambda$, this method cannot be used to estimate the test error of the whole procedure. Instead, the dataset has to be split into a training set and a testing set. Cross-validation done on the training set and the test error is estimated by evaluating the performance of the model on the testing set. However, this can yield significantly different estimates of the test error since only one split into training and testing data is considered. To get a better estimate of the actual testing error, a bootstrap procedure is performed.
	
	Since the full dataset is an iid sample from some unknown distribution, the estimated test error $\hat E_{new}$ is a random variable. By repeating the whole procedure $B$ times (i.e. $B$ independent splits into training and testing data with subsequent fitting and cross-validation), a bootstrap sample of $\hat E_{new}$ is obtained which can be used to obtain a better estimate of $E_{new}$. This is very computationally intensive if $B$ is large.
\end{document}