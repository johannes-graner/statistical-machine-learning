\documentclass[../../project.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}
	The $k$-nearest neighbors ($k$-NN) method is based on the simple principle of finding the $k$ closest neighboring points with respect to the input data $X \in \mathcal{X} \subseteq \mathbb{R}^p$. In the case of classification the outcome $Y$ is then determined by a majority vote among the $k$ nearest data points. The method is based on the idea that if a test data point is close to some training data point then the prediction should be that they have the same outcome $Y$.
	
	The algorithm for $k$-NN can be implemented in a simple manner with a brute force algorithm for measuring the distance from the test data point $\boldsymbol{x_{\star}}$ to each training data point $\boldsymbol{x_{i}}$, where $i = 1,...,n$ using some distance function $d(x,y)$. It is standard to use the Minkowski distance for a certain order $p$, depending on the problem, which is given by
	\begin{equation}
	d(\boldsymbol{x},\boldsymbol{y}) = \left( \sum_{i=1}^{n} |x_{i} - y_{i}|^{p} \right)^{\frac{1}{p}}\text{, where } \boldsymbol{x} = (x_1,...,x_p),\boldsymbol{y}=(y_1,...,y_p)
	\in \mathbb{R}^p.
	\end{equation}
	Note that $p=1$ gives the Manhattan distance, and $p=2$ gives the Euclidean distance. The brute force algorithm for $k$-NN is given by \citep{Kursbok}
	\par\noindent\rule{\textwidth}{0.4pt}
	\begin{enumerate}
		\item Calculate the distance $d(\boldsymbol{x_{i}},\boldsymbol{x_{\star}})$ for each $i = 1,...,n$
		\item Set $\mathcal{N}_{\star} = \{ \boldsymbol{x_i}: \textit{Where } \boldsymbol{x_i} \textit{ is one of the k nearest points} \}$
		\item Return $\hat{y}(\boldsymbol{x_{\star}}) = \text{MajorityVote}\{y_j : j \in \mathcal{N}_{\star}\} $
	\end{enumerate}
	\par\noindent\rule{\textwidth}{0.4pt}
	
	
	%%A problem with the brute force algorithm is that all the training data has to be stored and each distance has to be calculated which can be rather computer intensive. 
	A problem with the brute force algorithm is that for each point we need to calculate the distance to every other point, which is computationally demanding for larger datasets.
	
	There are however more computationally  efficient algorithms to find the k-NN compared to the brute force search such as ball-tree and k-d tree which are not explained in detail here. All three of these algorithms were tested and no significant difference in the results where noted thus the choice of algorithm was set to "auto" which chooses the best suited algorithm for a given problem, further described in the Scikit-learn documentation \citep{sklearn}. 
	
	For our problem we consider the Minkowski distance and let $p$ and $k$ be hyper-parameters which are to be tuned. This is done in an analogous manner to the case of finding the hyper-parameter $\lambda$ in the regularization problem with logistic regression previously considered.
	
	%%Weighted $k$-NN is an alternative approach to the normal $k$-NN where the $k$ nearest neigbors also are weighted based on how far or close from the test data point they actually are effecting the majority vote such that for example closer points have "stronger" vote. In our case we compared between \textit{uniform} weights (Standard $k$-NN) and \textit{distance} weights where the weight points equals
	
	An alternative approach is to use weighted $k$-NN. The idea is to let the distance of the training data point to the test data point influence the strength of the vote. We compared \textit{uniform weights} (standard $k$-NN, where all weights equal $1$) and \textit{distance weights} where the weights are given by 
	\begin{equation}
	\dfrac{1}{d(\boldsymbol{x_{\star}, \boldsymbol{x_i}})}, 
	\end{equation}
	for each of the $k$-nearest neighbors. This reinforces the idea that proximity of test data points to training data points ought be a good predictor \citep{sklearn2}.
\end{document}