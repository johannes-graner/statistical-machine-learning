\documentclass[../../project.tex]{subfiles}
\graphicspath{{\subfix{images/}}}

\begin{document}
	Logistic regression is a \textit{general linear model} (GLM), i.e. the relationship between the data $X \in \mathcal{X} \subseteq \mathbb{R}^p$ and the outcome $Y$ is on the form
	\begin{equation}
	E(Y|X=x) = g^{-1}(x \cdot \beta)
	\end{equation}
	where $\beta \in \mathbb{R}^p$ and $g$ is the link function. In the case of logistic regression, $Y|(X = x) \sim Ber(p(x))$ and the canonical link function is the logit link $g(x) = \log \left( \frac{x}{1 - x} \right)$ with $g^{-1}(x) = \frac{\exp(x)}{1 + \exp(x)}$. Since $Y|(X=x) \sim Ber(p(x))$, we get $E(Y|X=x) = p(x) = g^{-1}(x \cdot \beta)$. In other words, $P(Y = 1 | X = x) = g^{-1}(x \cdot \beta)$, which we can use to predict $Y$ given data $x$.
	
	To do the regression, we find $\hat \beta \in \argmin_\beta \sum_{i=1}^{n} (y_i - \hat y(x_i; \beta))^2$ where $\hat y(x;\beta) = g^{-1} (x \cdot \beta)$. This is the MLE estimator of $\beta$ this minimizes the mean squared error (MSE) loss function. A potential problem with this approach is that there are no restrictions on the components of $\beta$ and that can lead to overfitting, especially if $n$ is not much larger than $p$. To address that issue, one can introduce regularization.
	%%Kalla estimatorn $\hat{\beta}_{MLE}$ för att förtydliga varför den här estimatorn används?
	
	In general, regularization is done by adding a penalizing term to the loss function that restricts $\beta$ in some way. If $L(\beta; x_i,y_i)$ is the loss function before regularization, we instead consider the new loss function $L(\beta; x_i,y_i) + \lambda R(\beta)$ and find $\hat \beta_{reg} \in \argmin_\beta \left( L(\beta; x_i, y_i) + \lambda R(\beta) \right)$. $R$ is some penalizing function and $\lambda$ is a hyper-parameter that can be tuned. The two most common forms of regularization is LASSO and Ridge regression.
	
	LASSO regression uses $L_1$-regularization, meaning that $R_{LASSO}(\beta) = ||\beta||_1 =  \sum_{i=1}^{p} |\beta_i|$ while Ridge regression uses $L_2$-regularization, $R_{Ridge}(\beta) = ||\beta||_2^2 = \sum_{i=1}^{p} \beta_i^2$.
	
	%% Sista 3 styckena bör ligga i methods?
\end{document}